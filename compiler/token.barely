define TOKEN_OPEN_PARENTHESIS = 0;
define TOKEN_CLOSED_PARENTHESIS = 1;
define TOKEN_OPEN_CURLY_BRACKETS = 2;
define TOKEN_CLOSED_CURLY_BRACKETS = 3;
define TOKEN_COMMA = 4;
define TOKEN_SEMICOLON = 5;
define TOKEN_KEYWORD = 6;

define TokenKeywordData = structure {
    keyword: *;
};

define TOKEN_NAME = 7;

define TokenNameData = structure {
    name: *;
};

define TOKEN_STRING = 8;

define TokenStringData = structure {
    value: *;
};

define TOKEN_NUMBER = 9;

define TokenNumberData = structure {
    value: whole_8;
};

define TOKEN_COLON = 10;

define TOKEN_BOOLEAN = 11;

define TokenBooleanData = structure {
    value: boolean;
};

define TOKEN_OPEN_BRACKET = 12;
define TOKEN_CLOSED_BRACKET = 13;
define TOKEN_PERIOD = 14;

define TOKEN_SIZE = 56;

define TokenLocationData = structure {
    file: *;
    row: whole_8;
    column: whole_8;
};

define TOKEN_DATA_OFFSET = 32;

define Tokens = structure {
    buffer: AutoBuffer;
    index: whole_8;
};

define tokens_new = function(): Tokens {
    declare tokens: Tokens;

    Tokens.<buffer(&(tokens), autobuffer_new(131072));
    Tokens.<index(&(tokens), 0);

    return tokens;
};

define tokens_get_id = function(tokens: *Tokens, index: whole_8): whole_8 {
    return autobuffer_get_whole_8(&(Tokens.>buffer(tokens)), *(index, TOKEN_SIZE));
};

define tokens_get_pointer = function(tokens: *Tokens, index: whole_8): * {
    return autobuffer_get(&(Tokens.>buffer(tokens)), *(index, TOKEN_SIZE));
};

define tokens_get_pointer_data = function(tokens: *Tokens, index: whole_8): * {
    return autobuffer_get(&(Tokens.>buffer(tokens)), +(*(index, TOKEN_SIZE), TOKEN_DATA_OFFSET));
};

define tokens_get_location_data = function(tokens: *Tokens, index: whole_8): *TokenLocationData {
    return TokenLocationData.pcast(autobuffer_get(&(Tokens.>buffer(tokens)), +(*(index, TOKEN_SIZE), 8)));
};

define tokens_append_data = function(tokens: *Tokens, id: whole_8, file_name: *, row: whole_8, column: whole_8): * {
    return *.cast(+(tokens_append(tokens, id, file_name, row, column), TOKEN_DATA_OFFSET));
};

define tokens_append = function(tokens: *Tokens, id: whole_8, file_name: *, row: whole_8, column: whole_8): * {
    declare tokens_buffer = &(Tokens.>buffer(tokens));
    declare tokens_index = &(Tokens.>index(tokens));
    autobuffer_register(tokens_buffer, *(whole_8.>(tokens_index), TOKEN_SIZE), TOKEN_SIZE);
    declare to_return = autobuffer_get(tokens_buffer, *(whole_8.>(tokens_index), TOKEN_SIZE));

    autobuffer_set_whole_8(tokens_buffer, id, *(whole_8.>(tokens_index), TOKEN_SIZE));
    autobuffer_set_pointer(tokens_buffer, file_name, +(*(whole_8.>(tokens_index), TOKEN_SIZE), 8));
    autobuffer_set_whole_8(tokens_buffer, row, +(*(whole_8.>(tokens_index), TOKEN_SIZE), 16));
    autobuffer_set_whole_8(tokens_buffer, column, +(*(whole_8.>(tokens_index), TOKEN_SIZE), 24));

    increment_whole_8_pointer(tokens_index, 1);

    return to_return;
};

macro #tokens_append_data(tokens, id, type, file_name, row, column) {
    type.pcast(tokens_append_data(tokens, id, file_name, row, column))
};

define tokens_is_keyword = function(tokens: *Tokens, index: whole_8, keyword: *): boolean {
    return if =(tokens_get_id(tokens, index), TOKEN_KEYWORD) {
        string=(TokenKeywordData.>keyword(TokenKeywordData.pcast(tokens_get_pointer_data(tokens, index))), keyword)
    } else {
        false
    };
};

define tokens_is_name = function(tokens: *Tokens, index: whole_8, name: *): boolean {
    return if =(tokens_get_id(tokens, index), TOKEN_NAME) {
        string=(TokenNameData.>name(TokenNameData.pcast(tokens_get_pointer_data(tokens, index))), name)
    } else {
        false
    };
};
